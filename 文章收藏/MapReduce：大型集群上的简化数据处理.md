# MapReduce：大型集群上的简化数据处理


**作者：Jeffrey Dean and Sanjay Ghemawat**
jeff@google.com, sanjay@google.com
Google, Inc.

## 摘要

MapReduce 是一个用于处理和生成大数据集的编程模型及其关联实现。用户指定一个 `map`（映射）函数，该函数处理一个键/值对以生成一组中间键/值对；以及一个 `reduce`（归约）函数，该函数合并与同一中间键关联的所有中间值。如本文所示，许多现实世界的任务都可以用这个模型来表达。

以这种函数式风格编写的程序能够自动地在大型商用机器集群上实现并行化并执行。运行时系统负责处理输入数据的分区、在机器集群上调度程序的执行、处理机器故障以及管理所需的机器间通信等细节。这使得没有并行和分布式系统经验的程序员也能轻松利用大型分布式系统的资源。

我们的 MapReduce 实现运行在一个大型商用机器集群上，并且具有高度的可扩展性：一个典型的 MapReduce 计算可以在数千台机器上处理数TB的数据。程序员发现该系统易于使用：已经实现了数百个 MapReduce 程序，每天在 Google 的集群上执行超过一千个 MapReduce 作业。

## 1 简介

在过去的五年里，作者和 Google 的许多其他人已经实现了数百个专门用于处理大量原始数据的计算，例如爬取的文档、Web 请求日志等，以计算各种派生数据，例如倒排索引、Web 文档图结构的各种表示、每台主机爬取的页面数量摘要、一天中最频繁查询的集合等。大多数这类计算在概念上是直接的。然而，输入数据通常很大，计算必须分布在数百或数千台机器上才能在合理的时间内完成。如何并行化计算、分发数据和处理故障等问题，使得原本简单的计算被大量复杂的代码所掩盖，以处理这些问题。

为了应对这种复杂性，我们设计了一种新的抽象模型。它允许我们表达我们试图执行的简单计算，但隐藏了并行化、容错、数据分发和负载均衡等 messy 的细节。我们的抽象受到 Lisp 和许多其他函数式语言中存在的 `map` 和 `reduce` 原语的启发。我们意识到，我们的大多数计算都涉及对输入的每个逻辑“记录”应用 `map` 操作以计算一组中间键/值对，然后对共享相同键的所有值应用 `reduce` 操作，以适当地组合派生数据。我们使用带有用户指定的 `map` 和 `reduce` 操作的函数模型，这使我们能够轻松地并行化大型计算，并使用重新执行作为容错的主要机制。

这项工作的主要贡献是一个简单而强大的接口，它能够对大规模计算进行自动并行化和分发，并结合了一个在该接口上实现了商品化PC集群高性能的实现。

第2节描述了基本的编程模型并给出了一些例子。第3节描述了针对我们基于集群的计算环境量身定制的 MapReduce 接口的实现。第4节描述了我们发现有用的一些编程模型的改进。第5节是我们对各种任务的实现的性能测量。第6节探讨了 MapReduce 在 Google 内部的使用，包括我们将其作为重写我们生产索引系统的基础的经验。

## 2 编程模型

计算接受一组输入的键/值对，并产生一组输出的键/值对。MapReduce 库的用户将计算表示为两个函数：`Map` 和 `Reduce`。

由用户编写的 `Map` 函数接受一个输入对，并产生一组中间键/值对。MapReduce 库将与同一中间键 `I` 相关联的所有中间值组合在一起，并将它们传递给 `Reduce` 函数。

同样由用户编写的 `Reduce` 函数接受一个中间键 `I` 和该键的一组值。它将这些值合并在一起，形成一个可能更小的值集合。通常，每次 `Reduce` 调用只产生零个或一个输出值。中间值通过迭代器提供给用户的 `reduce` 函数。这使我们能够处理那些大到无法装入内存的值列表。

### 2.1 示例

考虑计算一个大型文档集合中每个单词出现次数的问题。用户会编写类似以下伪代码的代码：

```
map(String key, String value):
  // key: 文档名
  // value: 文档内容
  for each word w in value:
    EmitIntermediate(w, "1");

reduce(String key, Iterator values):
  // key: 一个单词
  // values: 一个计数的列表
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```

`map` 函数为每个单词发出一个关联的出现次数（在这个简单示例中就是 '1'）。`reduce` 函数将特定单词发出的所有计数加在一起。

此外，用户编写代码以填充一个 `mapreduce` 规范对象，其中包含输入和输出文件的名称以及可选的调优参数。然后用户调用 `MapReduce` 函数，将规范对象传递给它。用户的代码与 MapReduce 库（用 C++ 实现）链接在一起。

### 2.2 类型

尽管前面的伪代码是根据字符串输入和输出来编写的，但从概念上讲，用户提供的 `map` 和 `reduce` 函数具有关联的类型：

`map (k1,v1) → list(k2,v2)`
`reduce (k2,list(v2)) → list(v3)`

也就是说，输入键和值来自与输出键和值不同的域。此外，中间键和值与输出键和值的域相同。

我们的 C++ 实现将字符串传递给用户定义的函数，并由用户代码在字符串和适当的类型之间进行转换。

### 2.3 更多示例

这里有一些可以轻松地表示为 MapReduce 计算的有趣程序的简单示例。

* **分布式 Grep (Distributed Grep):** `map` 函数在匹配到提供的模式时发出一行。`reduce` 函数是一个恒等函数，它只是将提供的中间数据复制到输出。
* **URL 访问频率计数 (Count of URL Access Frequency):** `map` 函数处理网页请求日志并输出 `(URL, 1)`。`reduce` 函数将同一 URL 的所有值相加，并发出一个 `(URL, total_count)` 对。
* **反向网络链接图 (Reverse Web-Link Graph):** `map` 函数为在名为 `source` 的页面中找到的每个指向 `target` URL 的链接输出 `(target, source)` 对。`reduce` 函数连接与给定 `target` URL 关联的所有 `source` URL 的列表，并发出该对：`(target, list(source))`。
* **每个主机的术语向量 (Term-Vector per Host):** 术语向量将文档或文档集中最重要的单词总结为一个 `(word, frequency)` 对的列表。`map` 函数为每个输入文档发出一个 `(hostname, term_vector)` 对（其中主机名从文档的 URL 中提取）。`reduce` 函数被传递给定主机的所有文档的术语向量。它将这些术语向量相加，丢弃不常见的术语，然后发出一个最终的 `(hostname, term_vector)` 对。
* **倒排索引 (Inverted Index):** `map` 函数解析每个文档，并发出一系列 `(word, document ID)` 对。`reduce` 函数接受给定单词的所有对，对相应的文档 ID 进行排序，并发出一个 `(word, list(document ID))` 对。所有输出对的集合构成一个简单的倒排索引。很容易增强此计算以跟踪单词位置。
* **分布式排序 (Distributed Sort):** `map` 函数从每个记录中提取键，并发出一个 `(key, record)` 对。`reduce` 函数不加改变地发出所有对。此计算依赖于第 4.1 节中描述的分区功能和第 4.2 节中描述的排序属性。

## 3 实现

MapReduce 接口可以有多种不同的实现方式。正确的选择取决于环境。例如，一种实现可能适用于小型的共享内存机器，另一种适用于大型 NUMA 多处理器，还有一种适用于更大规模的网络机器集合。

本节描述了一个针对 Google 广泛使用的计算环境的实现：由交换式以太网连接的大型商用 PC 集群。在我们的环境中：

1. 机器通常是运行 Linux 的双核 x86 处理器，每台机器有 2-4 GB 内存。
2. 使用商用网络硬件——通常在机器级别为 100 Mbps 或 1 Gbps，但平均整体对分带宽要低得多。
3. 集群由成百上千台机器组成，因此机器故障是常态。
4. 存储由直接连接到单个机器的廉价 IDE 磁盘提供。内部开发的分布式文件系统 用于管理存储在这些磁盘上的数据。该文件系统使用复制来在不可靠的硬件之上提供可用性和可靠性。
5. 用户向调度系统提交作业。每个作业由一组任务组成，并由调度程序映射到集群内的一组可用机器上。

### 3.1 执行流程概览

`Map` 的调用通过自动将输入数据分区为 M 个分片（split）来分布在多台机器上。输入分片可以由不同的机器并行处理。`Reduce` 的调用通过使用分区函数（例如，`hash(key) mod R`）将中间键空间划分为 R 个部分来分布。分区的数量（R）和分区函数由用户指定。

图 1 显示了我们实现中 MapReduce 操作的总体流程。当用户程序调用 `MapReduce` 函数时，会发生以下一系列操作（图 1 中的编号标签与下面列表中的编号相对应）：

![图 1：执行流程概览](https://storage.googleapis.com/actlab-bucket/gemini_cache/e06b9968417d9fd2e1858c8c5c490807.png)

1. 用户程序中的 MapReduce 库首先将输入文件分割成 M 个片段，通常每个片段大小为 16MB 到 64MB（用户可通过可选参数控制）。然后，它在机器集群上启动程序的多个副本。
2. 程序的其中一个副本是特殊的——即 master。其余的是由 master 分配工作的 worker。有 M 个 map 任务和 R 个 reduce 任务需要分配。master 选择空闲的 worker，并为每个 worker 分配一个 map 任务或一个 reduce 任务。
3. 被分配了 map 任务的 worker 读取相应输入分片的内容。它从输入数据中解析出键/值对，并将每个对传递给用户定义的 `Map` 函数。由 `Map` 函数产生的中间键/值对被缓存在内存中。
4. 周期性地，缓存的键/值对被写入本地磁盘，并通过分区函数划分为 R 个区域。这些缓存在本地磁盘上的位置会被传回给 master，master 负责将这些位置转发给 reduce worker。
5. 当 reduce worker 收到 master 关于这些位置的通知时，它使用远程过程调用从 map worker 的本地磁盘读取缓存的数据。当 reduce worker 读取完所有中间数据后，它会按中间键对数据进行排序，以便将所有相同键的出现分组在一起。排序是必要的，因为通常许多不同的键会映射到同一个 reduce 任务。如果中间数据量太大而无法在内存中容纳，则会使用外部排序。
6. reduce worker 遍历排序后的中间数据，对于遇到的每个唯一的中间键，它将该键和相应的中间值集合传递给用户的 `Reduce` 函数。`Reduce` 函数的输出被附加到该 reduce 分区的最终输出文件中。
7. 当所有的 map 任务和 reduce 任务都完成后，master 唤醒用户程序。此时，用户程序中的 `MapReduce` 调用返回到用户代码。

成功完成后，mapreduce 执行的输出可在 R 个输出文件中找到（每个 reduce 任务一个文件，文件名由用户指定）。通常，用户不需要将这 R 个输出文件合并成一个文件——他们通常将这些文件作为输入传递给另一个 MapReduce 调用，或者从另一个能够处理分片输入的分布式应用程序中使用它们。

### 3.2 Master 数据结构

master 保存着几个数据结构。对于每个 map 任务和 reduce 任务，它存储其状态（空闲、进行中或已完成）和 worker 机器的标识（对于非空闲任务）。

master 是将中间文件区域的位置从 map 任务传播到 reduce 任务的通道。因此，对于每个已完成的 map 任务，master 存储由该 map 任务产生的 R 个中间文件区域的位置和大小。当 map 任务完成时，此位置和大小信息的更新会被接收。该信息被增量地推送给正在进行中的 reduce 任务的 worker。

### 3.3 容错

由于 MapReduce 库旨在帮助使用成百上千台机器处理非常大量的数据，因此该库必须能够优雅地处理机器故障。

**Worker 故障**

master 会周期性地 ping 每个 worker。如果在一定时间内没有收到来自 worker 的响应，master 会将该 worker 标记为失败。该 worker 完成的任何 map 任务都会被重置回其初始的空闲状态，因此可以被调度到其他 worker 上。类似地，在失败的 worker 上正在进行的任何 map 任务或 reduce 任务也会被重置为空闲状态，并变得可以重新调度。

已完成的 map 任务在失败时会被重新执行，因为它们的输出存储在失败机器的本地磁盘上，因此无法访问。已完成的 reduce 任务不需要重新执行，因为它们的输出存储在全局文件系统中。

当一个 map 任务首先由 worker A 执行，然后由于 A 失败而由 worker B 执行时，所有执行 reduce 任务的 worker 都会收到重新执行的通知。任何尚未从 worker A 读取数据的 reduce 任务将从 worker B 读取数据。

MapReduce 对大规模的 worker 故障具有弹性。例如，在一次 MapReduce 操作期间，一个正在运行的集群上的网络维护导致一组 80 台机器在几分钟内无法访问。MapReduce master 只是简单地重新执行了无法访问的 worker 机器所做的工作，并继续向前推进，最终完成了 MapReduce 操作。

**Master 故障**

让 master 定期写入其上述数据结构的检查点是很容易的。如果 master 任务死亡，可以从最后一个检查点状态启动一个新的副本。然而，考虑到只有一个 master，其失败是不太可能的；因此，我们当前的实现如果 master 失败就会中止 MapReduce 计算。客户端可以检查此条件并根据需要重试 MapReduce 操作。

**失败情况下的语义**

当用户提供的 `map` 和 `reduce` 操作是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的无故障顺序执行所产生的输出相同。

我们依靠 map 和 reduce 任务输出的原子提交来实现此属性。每个进行中的任务将其输出写入私有的临时文件。一个 reduce 任务产生一个这样的文件，一个 map 任务产生 R 个这样的文件（每个 reduce 任务一个）。当一个 map 任务完成时，worker 向 master 发送一条消息，并在消息中包含 R 个临时文件的名称。如果 master 收到一个已经完成的 map 任务的完成消息，它会忽略该消息。否则，它会在 master 数据结构中记录 R 个文件的名称。

当一个 reduce 任务完成时，reduce worker 会原子地将其临时输出文件重命名为最终输出文件。如果同一个 reduce 任务在多台机器上执行，将对同一个最终输出文件执行多个重命名调用。我们依靠底层文件系统提供的原子重命名操作来保证最终的文件系统状态只包含一次 reduce 任务执行所产生的数据。

绝大多数 map 和 reduce 操作是确定性的，在这种情况下，我们的语义等同于顺序执行，这一事实使得程序员很容易推理他们程序的行为。当 map 和/或 reduce 操作不确定时，我们提供较弱但仍然合理的语义。在存在不确定性操作的情况下，特定 reduce 任务 `R₁` 的输出等同于不确定性程序的顺序执行为 `R₁` 产生的输出。然而，另一个 reduce 任务 `R₂` 的输出可能对应于不确定性程序的不同顺序执行为 `R₂` 产生的输出。

### 3.4 局部性 (Locality)

在我们的计算环境中，网络带宽是一种相对稀缺的资源。我们通过利用输入数据（由 GFS 管理）存储在构成我们集群的机器的本地磁盘上的事实来节省网络带宽。GFS 将每个文件划分为 64MB 的块，并在不同的机器上存储每个块的几个副本（通常是 3 个）。MapReduce master 会考虑输入文件的位置信息，并尝试在包含相应输入数据副本的机器上调度 map 任务。如果失败，它会尝试在任务输入数据的副本附近调度一个 map 任务（例如，在与包含数据的机器位于同一网络交换机上的 worker 机器上）。当在一个集群中相当一部分 worker 上运行大型 MapReduce 操作时，大多数输入数据都是在本地读取的，不消耗网络带宽。

### 3.5 任务粒度 (Task Granularity)

如上所述，我们将 map 阶段细分为 M 个片段，将 reduce 阶段细分为 R 个片段。理想情况下，M 和 R 应该远大于 worker 机器的数量。让每个 worker 执行许多不同的任务可以改善动态负载均衡，并且在 worker 发生故障时也能加快恢复速度：它已完成的许多 map 任务可以分散到所有其他 worker 机器上。

在我们的实现中，M 和 R 的大小存在实际限制，因为 master 必须做出 `O(M + R)` 次调度决策，并保留 `O(M * R)` 的状态在内存中。（然而，内存使用的常数因子很小：`O(M * R)` 的状态由每个 map 任务/reduce 任务对大约一个字节的数据组成。）

此外，R 通常受到用户的限制，因为每个 reduce 任务的输出最终会放在一个单独的输出文件中。在实践中，我们倾向于选择 M，使得每个单独的任务处理大约 16MB 到 64MB 的输入数据（以便上述的局部性优化最有效），并且我们将 R 设置为我们期望使用的 worker 机器数量的小倍数。我们经常执行 M = 200,000 和 R = 5,000，使用 2,000 台 worker 机器的 MapReduce 计算。

### 3.6 备用任务 (Backup Tasks)

延长 MapReduce 操作总时间的常见原因之一是“掉队者”（straggler）：一台机器花费异常长的时间来完成计算中最后几个 map 或 reduce 任务之一。掉队者的出现原因有很多。例如，磁盘有问题的机器可能会经历频繁的可纠正错误，从而将其读取性能从 30 MB/s 降低到 1 MB/s。集群调度系统可能已在该机器上安排了其他任务，导致它因竞争 CPU、内存、本地磁盘或网络带宽而执行 MapReduce 代码的速度变慢。我们最近遇到的一个问题是机器初始化代码中的一个错误，导致处理器缓存被禁用：受影响机器上的计算速度减慢了一百多倍。

我们有一个通用的机制来缓解掉队者问题。当一个 MapReduce 操作接近完成时，master 会为剩余的进行中任务调度备用执行。无论主执行还是备用执行完成，任务都会被标记为已完成。我们调整了此机制，使其通常只会增加操作所用计算资源的几个百分点。我们发现，这大大减少了完成大型 MapReduce 操作的时间。例如，第 5.3 节中描述的排序程序在禁用备用任务机制时，完成时间要长 44%。

## 4 优化 (Refinements)

尽管简单地编写 `Map` 和 `Reduce` 函数提供的基本功能足以满足大多数需求，但我们发现一些扩展很有用。我们在本节中描述这些扩展。

### 4.1 分区函数 (Partitioning Function)

MapReduce 的用户指定他们想要的 reduce 任务/输出文件的数量（R）。数据通过对中间键使用分区函数在这些任务之间进行分区。提供了默认的分区函数，它使用哈希（例如，“hash(key) mod R”）。这往往会产生相当均衡的分区。然而，在某些情况下，通过键的其他函数对数据进行分区是很有用的。例如，有时输出键是 URL，我们希望同一主机的所有条目都最终在同一个输出文件中。为了支持这种情况，MapReduce 库的用户可以提供一个特殊的分区函数。例如，使用“hash(Hostname(urlkey)) mod R”作为分区函数，可以使来自同一主机的所有 URL 都最终进入同一个输出文件。

### 4.2 顺序保证 (Ordering Guarantees)

我们保证在给定的分区内，中间键/值对是按键的升序处理的。这种排序保证使得为每个分区生成一个排序的输出文件变得容易，这在输出文件格式需要支持按键进行高效的随机访问查找，或者用户发现对输出进行排序很方便时非常有用。

### 4.3 合并函数 (Combiner Function)

在某些情况下，每个 map 任务产生的中间键存在显著的重复，并且用户指定的 `Reduce` 函数是可交换和可结合的。一个很好的例子是第 2.1 节中的单词计数示例。由于词频倾向于遵循 Zipf 分布，每个 map 任务将产生成百上千个 `<the, 1>` 形式的记录。所有这些计数将通过网络发送到单个 reduce 任务，然后由 `Reduce` 函数相加产生一个数字。我们允许用户指定一个可选的 `Combiner` 函数，该函数在数据通过网络发送之前对其进行部分合并。

`Combiner` 函数在执行 map 任务的每台机器上执行。通常，相同的代码用于实现合并器和归约器函数。`reduce` 函数和 `combiner` 函数之间的唯一区别在于 MapReduce 库如何处理函数的输出。`reduce` 函数的输出被写入最终的输出文件。`combiner` 函数的输出被写入一个中间文件，该文件将被发送到一个 reduce 任务。

部分合并显著加快了某些类别的 MapReduce 操作。

### 4.4 输入和输出类型

MapReduce 库支持以多种不同格式读取输入数据。例如，“text”模式输入将每一行视为一个键/值对：键是文件中的偏移量，值是该行的内容。另一个常见的支持格式是存储按键排序的键/值对序列。每个输入类型实现都知道如何将自身拆分为有意义的范围以作为单独的 map 任务进行处理（例如，文本模式的范围拆分确保范围拆分仅在行边界发生）。用户可以通过提供一个简单的 `reader` 接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义的输入类型之一。

`reader` 不一定需要从文件中提供数据。例如，定义一个从数据库或内存中映射的数据结构中读取记录的 `reader` 是很容易的。

以类似的方式，我们支持一组用于以不同格式生成数据的输出类型，并且用户代码很容易添加对新输出类型的支持。

### 4.5 副作用 (Side-effects)

在某些情况下，MapReduce 的用户发现从他们的 map 和/或 reduce 操作中生成辅助文件作为额外输出很方便。我们依赖于应用程序编写者使此类副作用具有原子性和幂等性。通常，应用程序会写入一个临时文件，并在文件完全生成后原子地重命名它。

我们不为单个任务产生的多个输出文件的原子两阶段提交提供支持。因此，产生具有跨文件一致性要求的多个输出文件的任务应该是确定性的。这个限制在实践中从未成为问题。

### 4.6 跳过损坏记录 (Skipping Bad Records)

有时用户代码中的错误会导致 `Map` 或 `Reduce` 函数在某些记录上确定性地崩溃。这样的错误会阻止 MapReduce 操作完成。通常的做法是修复错误，但有时这是不可行的；也许错误在一个第三方库中，而源代码不可用。而且，有时忽略少数记录是可以接受的，例如在对一个大型数据集进行统计分析时。我们提供了一种可选的执行模式，其中 MapReduce 库检测导致确定性崩溃的记录，并跳过这些记录以继续向前推进。

每个 worker 进程都安装一个信号处理器来捕获段错误和总线错误。在调用用户 `Map` 或 `Reduce` 操作之前，MapReduce 库将参数的序列号存储在一个全局变量中。如果用户代码产生一个信号，信号处理器会发送一个包含该序列号的“最后喘息”UDP 数据包给 MapReduce master。当 master 在一个特定记录上看到不止一次失败时，它会在下一次重新执行相应的 `Map` 或 `Reduce` 任务时指示该记录应该被跳过。

### 4.7 本地执行 (Local Execution)

调试 `Map` 或 `Reduce` 函数中的问题可能很棘手，因为实际的计算发生在一个分布式系统中，通常在数千台机器上，工作分配决策由 master 动态做出。为了方便调试、性能分析和小型测试，我们开发了 MapReduce 库的另一种实现，它在本地机器上顺序执行 MapReduce 操作的所有工作。向用户提供控制，以便计算可以限制在特定的 map 任务上。用户使用一个特殊的标志调用他们的程序，然后可以轻松地使用他们认为有用的任何调试或测试工具（例如 gdb）。

### 4.8 状态信息 (Status Information)

master 运行一个内部 HTTP 服务器，并导出一组状态页面供人工查看。状态页面显示计算的进度，例如已完成多少任务、有多少任务正在进行中、输入字节数、中间数据字节数、输出字节数、处理速率等。页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算将花费多长时间，以及是否应该添加更多资源到计算中。这些页面还可以用来判断计算何时比预期慢得多。

此外，顶层状态页面显示哪些 worker 失败了，以及它们失败时正在处理哪些 map 和 reduce 任务。这些信息在尝试诊断用户代码中的错误时非常有用。

### 4.9 计数器 (Counters)

MapReduce 库提供了一个计数器功能来统计各种事件的发生次数。例如，用户代码可能想要计算已处理的总字数或索引的德语文档数量等。

要使用此功能，用户代码创建一个命名的计数器对象，然后在 `Map` 和/或 `Reduce` 函数中适当地递增计数器。例如：

```c++
Counter* uppercase;
uppercase = GetCounter("uppercase");

map(String name, String contents):
  for each word w in contents:
    if (IsCapitalized(w)):
      uppercase->Increment();
    EmitIntermediate(w, "1");
```

来自各个 worker 机器的计数器值会周期性地传播到 master（附加在 ping 响应中）。master 聚合来自成功的 map 和 reduce 任务的计数器值，并在 MapReduce 操作完成时将它们返回给用户代码。当前的计数器值也显示在 master 状态页面上，以便人们可以观察实时计算的进度。在聚合计数器值时，master 会消除同一 map 或 reduce 任务重复执行的影响，以避免重复计数。（重复执行可能来自我们使用备用任务以及因故障而重新执行任务。）

一些计数器值由 MapReduce 库自动维护，例如已处理的输入键/值对的数量和已生成的输出键/值对的数量。

用户发现计数器功能对于健全性检查 MapReduce 操作的行为很有用。例如，在某些 MapReduce 操作中，用户代码可能希望确保生成的输出对的数量与处理的输入对的数量完全相等，或者处理的德语文档的比例在总文档数的某个可容忍的范围内。

## 5 性能

在本节中，我们测量了在大型机器集群上运行的两个计算的 MapReduce 性能。一个计算在约 1TB 的数据中搜索特定模式。另一个计算对约 1TB 的数据进行排序。

这两个程序代表了用户编写的大部分真实程序的子集——一类程序将数据从一种表示形式混洗到另一种表示形式，另一类程序从大型数据集中提取少量感兴趣的数据。

### 5.1 集群配置

所有程序都在一个大约由 1800 台机器组成的集群上执行。每台机器有两个启用了超线程的 2GHz Intel Xeon 处理器，4GB 内存，两个 160GB IDE 磁盘和一个千兆以太网链路。机器被安排在一个两级树状交换网络中，根部大约有 100-200 Gbps 的聚合带宽。所有机器都在同一个托管设施中，因此任何一对机器之间的往返时间都小于一毫秒。

在 4GB 的内存中，大约有 1-1.5GB 被集群上运行的其他任务保留。程序在一个周末下午执行，当时 CPU、磁盘和网络大部分处于空闲状态。

### 5.2 Grep

grep 程序扫描 10^10 个 100 字节的记录，搜索一个相对罕见的三字符模式（该模式出现在 92,337 个记录中）。输入被分割成大约 64MB 的片段（M = 15000），整个输出放在一个文件中（R = 1）。

图 2 显示了计算随时间的进展。Y 轴显示了扫描输入数据的速率。随着越来越多的机器被分配到这个 MapReduce 计算中，速率逐渐增加，在分配了 1764 个 worker 时达到超过 30 GB/s 的峰值。当 map 任务完成时，速率开始下降，在大约 80 秒时达到零。整个计算从开始到结束大约需要 150 秒。这包括大约一分钟的启动开销。开销是由于将程序传播到所有 worker 机器，以及与 GFS 交互以打开 1000 个输入文件并获取局部性优化所需信息的延迟。

![图 2：数据传输速率随时间变化](https://storage.googleapis.com/actlab-bucket/gemini_cache/2d49e6f8a46e1076f7b99c71629737eb.png)

### 5.3 Sort

sort 程序对 10^10 个 100 字节的记录（约 1TB 数据）进行排序。该程序模仿了 TeraSort 基准测试。

排序程序包含不到 50 行的用户代码。一个三行的 `Map` 函数从文本行中提取一个 10 字节的排序键，并发射键和原始文本行作为中间键/值对。我们使用内置的 `Identity` 函数作为 `Reduce` 操作符。此函数将中间键/值对不变地作为输出键/值对传递。最终排序的输出被写入一组双向复制的 GFS 文件中（即，程序的输出写入 2TB）。

和之前一样，输入数据被分割成 64MB 的片段（M = 15000）。我们将排序后的输出分区到 4000 个文件中（R = 4000）。分区函数使用键的初始字节将其隔离到 R 个片段之一。

图 3(a) 显示了排序程序的正常执行进度。

* **左上图** 显示了读取输入的速率。速率峰值约为 13 GB/s，并在所有 map 任务在 200 秒前完成后迅速下降。请注意，输入速率低于 grep。这是因为排序 map 任务花费大约一半的时间和 I/O 带宽将中间输出写入其本地磁盘。grep 的相应中间输出大小可以忽略不计。
* **左中图** 显示了数据通过网络从 map 任务发送到 reduce 任务的速率。这种混洗（shuffling）在第一个 map 任务完成时立即开始。图中的第一个峰值是针对第一批大约 1700 个 reduce 任务（整个 MapReduce 被分配了大约 1700 台机器，每台机器最多同时执行一个 reduce 任务）。大约在计算开始 300 秒后，这些第一批 reduce 任务中的一些完成了，我们开始为剩余的 reduce 任务混洗数据。所有的混洗在大约 600 秒时完成。
* **左下图** 显示了由 reduce 任务将排序后的数据写入最终输出文件的速率。在第一个混洗周期结束和写入周期开始之间存在延迟，因为机器正忙于对中间数据进行排序。写入以大约 2-4 GB/s 的速率持续一段时间。所有的写入在大约 850 秒时完成。包括启动开销在内，整个计算耗时 891 秒。这与目前 TeraSort 基准测试报告的最佳结果 1057 秒相似。

有几点需要注意：由于我们的局部性优化，输入速率高于混洗速率和输出速率——大多数数据是从本地磁盘读取的，绕过了我们相对带宽受限的网络。混洗速率高于输出速率，因为输出阶段写入排序数据的两个副本（我们为可靠性和可用性原因制作了两个输出副本）。

![图 3：排序程序不同执行的数据传输速率](https://storage.googleapis.com/actlab-bucket/gemini_cache/5e1fc4a1329c2d1b7d5ac3a68393539e.png)

### 5.4 备用任务的影响

在图 3(b) 中，我们展示了禁用了备用任务的排序程序的执行情况。执行流程与图 3(a) 中所示的相似，只是有一个非常长的尾部，几乎没有写入活动。在 960 秒后，除了 5 个 reduce 任务外，所有任务都已完成。然而，这最后几个掉队者直到 300 秒后才完成。整个计算耗时 1283 秒，运行时间增加了 44%。

### 5.5 机器故障

在图 3(c) 中，我们展示了一次排序程序的执行，其中我们在计算开始几分钟后故意杀死了 1746 个 worker 进程中的 200 个。底层的集群调度器立即在这些机器上重新启动了新的 worker 进程（因为只杀死了进程，机器本身仍然正常运行）。

worker 的死亡表现为负的输入速率，因为一些先前完成的 map 工作消失了（因为相应的 map worker 被杀死了）并且需要重做。这部分 map 工作的重新执行发生得相对较快。整个计算在 933 秒内完成，包括启动开销（仅比正常执行时间增加了 5%）。

## 6 经验

我们在 2003 年 2 月编写了 MapReduce 库的第一个版本，并在 2003 年 8 月对其进行了重大增强，包括局部性优化、跨 worker 机器的任务执行动态负载均衡等。从那时起，我们惊喜地发现 MapReduce 库在我们所从事的问题类型上具有广泛的适用性。它已在 Google 内部的广泛领域中使用，包括：

* 大规模机器学习问题，
* 用于 Google News 和 Froogle 产品的聚类问题，
* 用于生成热门查询报告（例如 Google Zeitgeist）的数据提取，
* 为新实验和产品提取网页属性（例如，从大量网页中提取地理位置用于本地化搜索），以及
* 大规模图计算。

图 4 显示了随着时间的推移，我们主要源代码管理系统中独立 MapReduce 程序数量的显著增长，从 2003 年初的 0 个增长到 2004 年 9 月下旬的近 900 个独立实例。MapReduce 之所以如此成功，是因为它使得编写一个简单的程序并在半小时内在数千台机器上高效运行成为可能，极大地加快了开发和原型制作周期。此外，它还允许没有分布式和/或并行系统经验的程序员轻松利用大量资源。

![图 4：MapReduce 实例随时间增长](https://storage.googleapis.com/actlab-bucket/gemini_cache/e3df5f04b1263c9b782975be35da2d88.png)

在每个作业结束时，MapReduce 库会记录有关作业所用计算资源的统计信息。在表 1 中，我们展示了 2004 年 8 月在 Google 运行的 MapReduce 作业子集的一些统计数据。

**表 1：2004 年 8 月运行的 MapReduce 作业**

| 指标 | 值 |
| --- | --- |
| 作业数量 | 29,423 |
| 平均作业完成时间 | 634 秒 |
| 使用的机器天数 | 79,186 天 |
| 读取的输入数据 | 3,288 TB |
| 产生的中间数据 | 758 TB |
| 写入的输出数据 | 193 TB |
| 每个作业的平均 worker 机器数 | 157 |
| 每个作业的平均 worker 死亡数 | 1.2 |
| 平均 map 任务数 | 3,351 |
| 平均 reduce 任务数 | 55 |
| 唯一的 map 实现 | 395 |
| 唯一的 reduce 实现 | 269 |
| 唯一的 map/reduce 组合 | 426 |

### 6.1 大规模索引

到目前为止，我们最重要的 MapReduce 应用之一是完全重写了产生 Google 网页搜索服务所用数据结构的生产索引系统。索引系统将我们爬虫系统检索到的大量文档作为输入，这些文档存储为一组 GFS 文件。这些文档的原始内容超过 20 TB。索引过程作为一系列五到十个 MapReduce 操作运行。使用 MapReduce（而不是之前索引系统中的 ad-hoc 分布式传递）带来了几个好处：

* 索引代码更简单、更小、更易于理解，因为处理容错、分发和并行化的代码隐藏在 MapReduce 库中。例如，当用 MapReduce 表示时，计算的一个阶段的大小从大约 3800 行 C++ 代码减少到大约 700 行。
* MapReduce 库的性能足够好，以至于我们可以将概念上不相关的计算分开，而不是将它们混合在一起以避免额外的数据传递。这使得更改索引过程变得容易。例如，在我们的旧索引系统中需要几个月才能完成的一项更改，在新系统中只需几天即可实现。
* 索引过程变得更容易操作，因为由机器故障、慢速机器和网络抖动引起的大多数问题都由 MapReduce 库自动处理，无需操作员干预。此外，通过向索引集群添加新机器，可以轻松提高索引过程的性能。

## 7 相关工作

许多系统提供了受限的编程模型，并利用这些限制来自动并行化计算。例如，可以在 N 个处理器上使用并行前缀计算，在 log N 时间内计算 N 元数组的所有前缀上的关联函数。MapReduce 可以被看作是基于我们处理大型现实世界计算的经验，对其中一些模型进行的简化和提炼。更重要的是，我们提供了一个可扩展到数千个处理器的容错实现。相比之下，大多数并行处理系统只在较小规模上实现，并将处理机器故障的细节留给了程序员。

Bulk Synchronous Programming 和一些 MPI 原语提供了更高级别的抽象，使程序员更容易编写并行程序。这些系统与 MapReduce 的一个关键区别在于，MapReduce 利用受限的编程模型来自动并行化用户程序并提供透明的容错能力。

我们的局部性优化借鉴了诸如 active disks 之类的技术，其中计算被推送到靠近本地磁盘的处理单元，以减少通过 I/O 子系统或网络发送的数据量。我们在直接连接了少量磁盘的商用处理器上运行，而不是直接在磁盘控制器处理器上运行，但总体方法是相似的。

我们的备用任务机制类似于 Charlotte 系统中采用的急切调度机制。简单急切调度的一个缺点是，如果给定任务导致重复失败，整个计算将无法完成。我们通过跳过损坏记录的机制修复了这个问题的一些实例。

## 8 结论

MapReduce 编程模型已在 Google 成功用于多种目的。我们将这一成功归因于几个原因。首先，该模型易于使用，即使对于没有并行和分布式系统经验的程序员也是如此，因为它隐藏了并行化、容错、局部性优化和负载均衡的细节。其次，各种各样的问题都可以很容易地表示为 MapReduce 计算。例如，MapReduce 用于生成 Google 生产 Web 搜索服务的数据、用于排序、用于数据挖掘、用于机器学习以及许多其他系统。第三，我们开发了一个可扩展到由数千台机器组成的大型集群的 MapReduce 实现。该实现有效地利用了这些机器资源，因此适用于 Google 遇到的许多大型计算问题。

我们从这项工作中吸取了一些教训。首先，限制编程模型使得并行化和分发计算以及使此类计算具有容错性变得容易。其次，网络带宽是一种稀缺资源。因此，我们系统中的许多优化都旨在减少通过网络发送的数据量：局部性优化使我们能够从本地磁盘读取数据，而将中间数据的单个副本写入本地磁盘可以节省网络带宽。第三，冗余执行可用于减少慢速机器的影响，并处理机器故障和数据丢失。

## A 词频统计

本节包含一个程序，用于计算命令行指定的输入文件集中每个唯一单词的出现次数。

```cpp
#include "mapreduce/mapreduce.h"

// 用户的 map 函数
class WordCounter : public Mapper {
 public:
  virtual void Map(const MapInput& input) {
    const string& text = input.value();
    const int n = text.size();
    for (int i = 0; i < n; ) {
      // 跳过前导空格
      while ((i < n) && isspace(text[i]))
        i++;
      
      // 找到单词结尾
      int start = i;
      while ((i < n) && !isspace(text[i]))
        i++;
      
      if (start < i)
        Emit(text.substr(start, i-start), "1");
    }
  }
};
REGISTER_MAPPER(WordCounter);

// 用户的 reduce 函数
class Adder : public Reducer {
  virtual void Reduce(ReduceInput* input) {
    // 迭代具有相同键的所有条目并对值求和
    int64 value = 0;
    while (!input->done()) {
      value += StringToInt(input->value());
      input->NextValue();
    }
    
    // 发出 input->key() 的总和
    Emit(IntToString(value));
  }
};
REGISTER_REDUCER(Adder);

int main(int argc, char** argv) {
  ParseCommandLineFlags(argc, argv);
  
  MapReduceSpecification spec;
  
  // 将输入文件列表存储到 "spec" 中
  for (int i = 1; i < argc; i++) {
    MapReduceInput* input = spec.add_input();
    input->set_format("text");
    input->set_filepattern(argv[i]);
    input->set_mapper_class("WordCounter");
  }
  
  // 指定输出文件
  MapReduceOutput* out = spec.output();
  out->set_filebase("/gfs/test/freq");
  out->set_num_tasks(100);
  out->set_format("text");
  out->set_reducer_class("Adder");
  
  // 可选：在 map 任务内进行部分求和以节省网络带宽
  out->set_combiner_class("Adder");
  
  // 调优参数：最多使用 2000 台机器，每个任务 100MB 内存
  spec.set_machines(2000);
  spec.set_map_megabytes(100);
  spec.set_reduce_megabytes(100);
  
  // 现在运行它
  MapReduceResult result;
  if (!MapReduce(spec, &result)) abort();
  
  // 完成：'result' 结构包含有关计数器、花费时间、使用的机器数等信息。
  return 0;
}
```

